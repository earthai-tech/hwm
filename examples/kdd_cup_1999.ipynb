{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792142ac",
   "metadata": {},
   "source": [
    "# 📄 Detailed Example Notebook\n",
    "\n",
    "Below is the complete example notebook with all the ``detailed_hwm_vs_lstm.ipynb`` cells combined for ease of use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bd17a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.23.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\daniel\\anaconda3\\envs\\watex\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     RandomizedSearchCV, train_test_split\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, StandardScaler\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!pip install tensorflow\n",
    "\n",
    "\"\"\"\n",
    "Adaptive Hammerstein-Wiener and LSTM Modeling on KDD Cup Dataset\n",
    "===============================================================\n",
    "\n",
    "This example demonstrates the use of the HWM toolkit for adaptive dynamic system \n",
    "modeling by applying both the Hammerstein-Wiener classifier and an LSTM neural \n",
    "network to the KDD Cup 1999 dataset. The goal is to classify network intrusions \n",
    "and evaluate the performance of intelligent models in handling complex, nonlinear \n",
    "relationships within the data.\n",
    "\n",
    "The workflow includes:\n",
    "1. **Data Loading and Resampling**: Loading the KDD Cup dataset and resampling \n",
    "   to a manageable size for efficient processing.\n",
    "2. **Data Preprocessing**: Scaling numerical features and encoding categorical \n",
    "   variables to prepare the data for modeling.\n",
    "3. **Model Training with Hammerstein-Wiener Classifier**: Utilizing the \n",
    "   `HammersteinWienerClassifier` for classification tasks.\n",
    "4. **Hyperparameter Tuning**: Applying `RandomizedSearchCV` to optimize model \n",
    "   parameters.\n",
    "5. **Evaluation and Visualization**: Assessing model performance using accuracy, \n",
    "   prediction stability score (PSS), and time-weighted accuracy (TWA), along with \n",
    "   plotting confusion matrices and ROC curves.\n",
    "6. **LSTM Model Training**: Implementing an LSTM neural network to handle sequence-based \n",
    "   data and comparing its performance with the Hammerstein-Wiener classifier.\n",
    "\n",
    "This example provides practical insights into building and evaluating intelligent \n",
    "network models using HWM and TensorFlow's Keras API.\n",
    "\n",
    "Author: Daniel\n",
    "Created on: Fri Nov  1 17:36:16 2024\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, auc, confusion_matrix, roc_curve, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from hwm.estimators import HammersteinWienerClassifier\n",
    "from hwm.metrics import prediction_stability_score, twa_score\n",
    "from hwm.utils import resample_data\n",
    "\n",
    "# Set the data path\n",
    "data_path = r'F:\\repositories'\n",
    "\n",
    "# Define column names as per KDD Cup 1999 dataset\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "\n",
    "# Define continuous and categorical features\n",
    "continuous_features = [\n",
    "    'duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot',\n",
    "    'num_failed_logins', 'num_compromised', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'count', 'srv_count', 'serror_rate',\n",
    "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'protocol_type', 'service', 'flag', 'land', 'logged_in',\n",
    "    'is_host_login', 'is_guest_login', 'root_shell', 'su_attempted',\n",
    "    'num_outbound_cmds'\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\n",
    "    os.path.join(data_path, 'kddcup.data_10_percent_corrected'),\n",
    "    names=column_names,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "# Resample the dataset to 100,000 samples for efficiency\n",
    "data = resample_data(data, samples=100000, random_state=42)\n",
    "\n",
    "# Encode the target variable: 0 for 'normal.', 1 for any attack\n",
    "data['label'] = data['label'].apply(lambda x: 0 if x == 'normal.' else 1)\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y.values, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define a custom ReLU transformer for the Hammerstein-Wiener model\n",
    "class ReLUTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer that applies the ReLU activation function.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method. Returns self.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "# Initialize the Hammerstein-Wiener Classifier\n",
    "hw_model = HammersteinWienerClassifier(\n",
    "    nonlinear_input_estimator=ReLUTransformer(),\n",
    "    nonlinear_output_estimator=ReLUTransformer(),\n",
    "    p=9,\n",
    "    loss=\"cross_entropy\",\n",
    "    time_weighting=\"linear\",\n",
    "    batch_size=\"auto\",\n",
    "    optimizer='sgd',\n",
    "    learning_rate=0.001,\n",
    "    max_iter=173, \n",
    "    early_stopping=True,\n",
    "    verbose=1, \n",
    ")\n",
    "\n",
    "# Train the Hammerstein-Wiener Classifier\n",
    "hw_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'p': randint(1, 10),  # Dependency order from 1 to 10\n",
    "    'batch_size': randint(32, 128),  # Batch size between 32 and 128\n",
    "    'optimizer': ['sgd', 'adam', 'adagrad'],  # Optimizers to choose from\n",
    "    'learning_rate': uniform(0.0001, 0.01),  # Learning rate from 0.0001 to 0.01\n",
    "    'max_iter': randint(50, 200)  # Max iterations between 50 and 200\n",
    "}\n",
    "\n",
    "# Initialize the Hammerstein-Wiener Classifier with fixed components\n",
    "fixed_hw_model = HammersteinWienerClassifier(\n",
    "    nonlinear_input_estimator=ReLUTransformer(),\n",
    "    nonlinear_output_estimator=ReLUTransformer(),\n",
    "    loss=\"cross_entropy\",\n",
    "    time_weighting=\"linear\",\n",
    "    verbose=0, \n",
    "    batch_size=200, \n",
    "    early_stopping=True, \n",
    ")\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=fixed_hw_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of parameter settings sampled\n",
    "    scoring='accuracy',  # Evaluation metric\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to find the best parameters\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_hw_model = random_search.best_estimator_\n",
    "y_pred_hw = best_hw_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Hammerstein-Wiener Classifier\n",
    "accuracy_hw = accuracy_score(y_test, y_pred_hw)\n",
    "y_pred_proba_hw = best_hw_model.predict_proba(X_test)[:, 1]\n",
    "pss_hw = prediction_stability_score(y_pred_proba_hw)\n",
    "twa_hw = twa_score(y_test, y_pred_hw, alpha=0.9)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Hammerstein-Wiener Classifier Accuracy: {accuracy_hw:.4f}\")\n",
    "print(f\"Hammerstein-Wiener Classifier PSS: {pss_hw:.4f}\")\n",
    "print(f\"Hammerstein-Wiener Classifier TWA: {twa_hw:.4f}\")\n",
    "\n",
    "def plot_results(y_true, y_pred, y_pred_proba, title):\n",
    "    \"\"\"\n",
    "    Plots the Confusion Matrix and ROC Curve for the given predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True target values.\n",
    "    y_pred : array-like\n",
    "        Predicted target values.\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class.\n",
    "    title : str\n",
    "        Title for the plots.\n",
    "    \"\"\"\n",
    "    # Confusion Matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc_score = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'{title} (AUC = {roc_auc_score:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for Hammerstein-Wiener Classifier\n",
    "plot_results(y_test, y_pred_hw, y_pred_proba_hw, 'Hammerstein-Wiener Classifier')\n",
    "\n",
    "# Determine the number of features\n",
    "n_features = X_processed.shape[1]\n",
    "\n",
    "# Define the number of timesteps\n",
    "timesteps = 9  # Should match the 'p' parameter used in Hammerstein-Wiener model\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Creates input sequences and corresponding targets for LSTM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        Feature matrix.\n",
    "    y : ndarray\n",
    "        Target vector.\n",
    "    timesteps : int\n",
    "        Number of timesteps for each input sequence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_seq : ndarray\n",
    "        Array of input sequences.\n",
    "    y_seq : ndarray\n",
    "        Array of target values corresponding to each sequence.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_seq.append(X[i:i + timesteps])\n",
    "        y_seq.append(y[i + timesteps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, timesteps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, timesteps)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f'X_train_seq shape: {X_train_seq.shape}')\n",
    "print(f'y_train_seq shape: {y_train_seq.shape}')\n",
    "print(f'X_test_seq shape: {X_test_seq.shape}')\n",
    "print(f'y_test_seq shape: {y_test_seq.shape}')\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, n_features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Display the model architecture\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the LSTM Model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "y_pred_proba_lstm = lstm_model.predict(X_test_seq).flatten()\n",
    "y_pred_lstm = (y_pred_proba_lstm >= 0.5).astype(int)\n",
    "pss_lstm = prediction_stability_score(y_pred_proba_lstm)\n",
    "twa_lstm = twa_score(y_test_seq, y_pred_lstm, alpha=0.9)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"LSTM PSS: {pss_lstm:.4f}\")\n",
    "print(f\"LSTM TWA: {twa_lstm:.4f}\")\n",
    "\n",
    "# Plot results for LSTM Model\n",
    "plot_results(y_test_seq, y_pred_lstm, y_pred_proba_lstm, 'LSTM Model')\n",
    "\n",
    "# Compute ROC curve for Hammerstein-Wiener Classifier\n",
    "fpr_hw, tpr_hw, _ = roc_curve(y_test, y_pred_proba_hw)\n",
    "roc_auc_hw = auc(fpr_hw, tpr_hw)\n",
    "\n",
    "# Compute ROC curve for LSTM Model\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_proba_lstm)\n",
    "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "# Plot both ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr_hw, tpr_hw, label=f'Hammerstein-Wiener (AUC = {roc_auc_hw:.4f})')\n",
    "plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_lstm:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Print Summary of Results\n",
    "print(\"Summary of Results:\")\n",
    "print(f\"Hammerstein-Wiener Classifier Accuracy: {accuracy_hw:.4f}\")\n",
    "print(f\"Hammerstein-Wiener Classifier PSS: {pss_hw:.4f}\")\n",
    "print(f\"Hammerstein-Wiener Classifier TWA: {twa_hw:.4f}\")\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"LSTM PSS: {pss_lstm:.4f}\")\n",
    "print(f\"LSTM TWA: {twa_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea863b9",
   "metadata": {},
   "source": [
    "# 📄 Example Notebook Breakdown\n",
    "\n",
    "Below is a detailed breakdown of each section in the notebook, explaining the purpose and functionality.\n",
    "\n",
    "## 📦 Importing Necessary Libraries\n",
    "\n",
    "We begin by importing all the required libraries and modules. This includes standard libraries for data manipulation and visualization, as well as specific modules from Scikit-learn, TensorFlow's Keras, and the HWM toolkit.\n",
    "\n",
    "## 🗄️ Setting the Data Path\n",
    "\n",
    "Specify the file path where the KDD Cup dataset is stored. Ensure that the path is correct to avoid file not found errors.\n",
    "\n",
    "## 📁 Loading the KDD Cup 1999 Dataset\n",
    "\n",
    "Load the dataset using Pandas, defining appropriate column names. We also categorize the features into continuous and categorical for preprocessing purposes.\n",
    "\n",
    "## 🔄 Resampling the Dataset\n",
    "\n",
    "To manage computational resources effectively, we resample the dataset to 100,000 samples using the `resample_data` utility from HWM. This reduces the dataset size while maintaining its statistical properties.\n",
    "\n",
    "## 🛠️ Data Preprocessing\n",
    "\n",
    "Preprocess the data by scaling numerical features using `StandardScaler` and encoding categorical variables using `OneHotEncoder`. This prepares the data for efficient modeling.\n",
    "\n",
    "## 🔀 Splitting the Data into Training and Testing Sets\n",
    "\n",
    "Split the preprocessed data into training and testing sets, ensuring that the split is stratified based on the target variable to maintain class distribution.\n",
    "\n",
    "## 🔧 Defining a Custom ReLU Transformer\n",
    "\n",
    "Define a custom transformer `ReLUTransformer` that applies the ReLU activation function. This transformer will be used in the Hammerstein-Wiener classifier to introduce nonlinearity.\n",
    "\n",
    "## 🏋️‍♂️ Initializing the Hammerstein-Wiener Classifier\n",
    "\n",
    "Initialize the `HammersteinWienerClassifier` with the custom ReLU transformers and specified parameters such as dependency order (`p`), loss function, optimizer, learning rate, and early stopping.\n",
    "\n",
    "## 🏋️‍♀️ Training the Hammerstein-Wiener Classifier\n",
    "\n",
    "Train the initialized Hammerstein-Wiener classifier using the training data.\n",
    "\n",
    "## 🎛️ Hyperparameter Tuning with RandomizedSearchCV\n",
    "\n",
    "Perform hyperparameter tuning using `RandomizedSearchCV` to identify the best combination of parameters that maximize the classifier's performance. This involves searching over parameters like dependency order, batch size, optimizer type, learning rate, and maximum iterations.\n",
    "\n",
    "## 📊 Evaluating the Hammerstein-Wiener Classifier\n",
    "\n",
    "Use the best estimator from the hyperparameter tuning to make predictions on the test set. Evaluate the model's performance using metrics such as accuracy, prediction stability score (PSS), and time-weighted accuracy (TWA).\n",
    "\n",
    "## 🖼️ Plotting Results\n",
    "\n",
    "Define a function `plot_results` to visualize the Confusion Matrix and ROC Curve for the models. This aids in understanding the classifier's performance in more detail.\n",
    "\n",
    "## 📈 Plotting Hammerstein-Wiener Classifier Results\n",
    "\n",
    "Visualize the performance of the Hammerstein-Wiener classifier using the `plot_results` function.\n",
    "\n",
    "## 🧠 Defining and Training the LSTM Model\n",
    "\n",
    "Implement an LSTM neural network to handle sequence-based data. This involves creating input sequences, building and compiling the LSTM model, and training it using the training sequences.\n",
    "\n",
    "## 🏗️ Building the LSTM Model\n",
    "\n",
    "Build and compile the LSTM model using TensorFlow's Keras API, specifying the architecture and compilation parameters.\n",
    "\n",
    "## 🏋️‍♂️ Training the LSTM Model\n",
    "\n",
    "Train the LSTM model using the training sequences, incorporating early stopping to prevent overfitting.\n",
    "\n",
    "## 📊 Evaluating the LSTM Model\n",
    "\n",
    "Evaluate the trained LSTM model on the test sequences and compute relevant metrics such as accuracy, prediction stability score (PSS), and time-weighted accuracy (TWA).\n",
    "\n",
    "## 📈 Plotting LSTM Model Results\n",
    "\n",
    "Visualize the performance of the LSTM model using the `plot_results` function.\n",
    "\n",
    "## 🆚 Comparing ROC Curves Between Models\n",
    "\n",
    "Compare the ROC curves of both the Hammerstein-Wiener classifier and the LSTM model to assess their performance relative to each other.\n",
    "\n",
    "## 📝 Summary of Results\n",
    "\n",
    "Print a summary of the performance metrics of both models for easy comparison.\n",
    "\n",
    "---\n",
    "\n",
    "# 📚 Additional Resources\n",
    "\n",
    "For more examples and detailed explanations, refer to the [HWM Documentation](https://hwm.readthedocs.io/en/latest/).\n",
    "\n",
    "# 📄 Conclusion\n",
    "\n",
    "This notebook showcases how to leverage the HWM toolkit alongside traditional machine learning models like LSTM to perform adaptive dynamic system modeling. By following the structured workflow, users can efficiently preprocess data, train sophisticated models, perform hyperparameter tuning, and evaluate model performance using advanced metrics and visualization techniques.\n",
    "\n",
    "Feel free to explore and extend this example to suit your specific machine learning and data analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83363af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
